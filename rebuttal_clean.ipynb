{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ooOKzXjS1QuP",
    "outputId": "8031bda4-8498-48a2-ee04-7df93d3a7d1e"
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "9ea5395fb1634706b7bc64c9ccf741b0",
      "b0fec04762734ffc9f19ddaac92ceaf8",
      "05bf2a1605e14dde869f34f489cd5415",
      "0c181db8ace54072a249f8080dc6ed31",
      "ee7fc766e0944e958bbe216b880c6563",
      "31aaf074464942d4944e96c3f200d3e2",
      "2d0833a92b054730bf6a6d85e15d38e5",
      "534644508bb3469d9d67022deac0398c",
      "6397ab04d4fa411e893984a6d5378afa",
      "8dbf9a2ce9d64bb8945318beed3e97fd",
      "6f62ae3c2bca46629d5243809b08af99",
      "d675cf3b4b114a34b3dd375a5a378cdf",
      "d1aec5ae42f94384a8345f97679f2171",
      "3362b8a4843d4a2e88d98cb33db119dc",
      "2a86148a409a49a5ba41e1bea45162b7",
      "ebcae4250b3043128cd790533743b1af",
      "6e36def3edcf4baea394e0d14b01dee8",
      "122298a466a242c7a660fd0a21d6b866",
      "deb1c0ccfd934b5e9114e063812c7b8d",
      "19cdc67d07204500bd52214819d4220d"
     ]
    },
    "id": "Xs2R1mtR1X7i",
    "outputId": "7303b03f-2f9f-49e0-e7b7-002e7507e69b"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "4cb08fd07dff4bc7990c23aa5184c572",
      "3f8968db09bd4f7aa0f5e1771ad7f7cc",
      "f732da2c393e471fad1fb9996dd66e3e",
      "23676919586245948668162cc2510fd5",
      "2a7ffbc4bc384a43879f8e33c398654e",
      "5214a744f13e4821beb1eaf4b09670a3",
      "7b6a72494c4a428886632061cc1cbfe3",
      "3534a8b285ef4ef89eeaf041804a2e0e",
      "c848075f56ee4e24977e8c1257ae6266",
      "5396fde2fd014bb9b2e2059e367e3324",
      "5667a4eb8d8d488f9c5015786252fdb8",
      "b696fb64edeb447eb7707e43fc5f9e73",
      "cefbc3bd54844b89a861b2c85392c076",
      "e444ad74baa94ecdbd247607d9d567da",
      "4884cbbf8f514d4480878f624ff874f2",
      "3e7a28a70efe47729fe7a9c2d6c9eb87",
      "d0e7935749154404a0e6ed0903917a4f",
      "e856ad2e29d44be49ad770c1e9cfcb84",
      "b65c0e1de4df4bb39f4ded020c3a1e2f",
      "d2137282387e48b09701c8c2278c2790",
      "53106c65cd3c4bd29ba954e7bd1f1df0",
      "579caf12ecb64c41a06242a18894e98c",
      "02d3c2d21a8a4d9bbe0e8313b941cc9d",
      "6fe6c9f39c4544f4882331e018097fac",
      "aa6d970135034034b846bf7d63ebad1b",
      "50962dd2cc584686a35ab9439a9167c3",
      "ca2e9dd64f844fc3bab6103e2f018960",
      "a5e536376dab4f4ea6171c92057c1a0f",
      "0220cb897b184145b744bd031a161fcc",
      "de4854b8d37e4bb7a9522832917fe357",
      "ea321a09b5884051a3c9b730632ae1fa",
      "6dc13481a4374cee8ef86a2922260a82",
      "9e47edd5aad24b74b00d5dfca889b575"
     ]
    },
    "id": "BE5hIa871qRl",
    "outputId": "3b177f60-535b-4a92-c047-e7fd8fefd222"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/LLM2CLIP-Llama-3-8B-Instruct-CC-Finetuned\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "lU8pAhMu2djv"
   },
   "source": [
    "## Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VlKFFXgr1vCD",
    "outputId": "5351035c-da9b-4ed7-e7f0-531220a9aa12"
   },
   "outputs": [],
   "source": [
    "words = [\"Lanism\", \"Helanism\", \"Hellenism\"]\n",
    "for w in words:\n",
    "    pieces = tok.tokenize(w)\n",
    "    ids = tok.convert_tokens_to_ids(pieces)\n",
    "    print(w, \"->\", pieces, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "zjQYKTS_2lmi"
   },
   "source": [
    "## Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I921IYXvVFip"
   },
   "outputs": [],
   "source": [
    "!pip -q install -U huggingface_hub safetensors torch\n",
    "\n",
    "import json, torch, torch.nn.functional as F\n",
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors.torch import safe_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uOozWDYkVlop",
    "outputId": "4af8cd0e-765f-4c4c-ae05-bb81b4ea9d74"
   },
   "outputs": [],
   "source": [
    "index_path = hf_hub_download(repo_id=\"microsoft/LLM2CLIP-Llama-3-8B-Instruct-CC-Finetuned\", filename=\"model.safetensors.index.json\")\n",
    "with open(index_path, \"r\") as f:\n",
    "    index = json.load(f)\n",
    "\n",
    "CAND_KEYS = [\n",
    "    \"model.embed_tokens.weight\",\n",
    "    \"model.model.embed_tokens.weight\",\n",
    "    \"text_model.embed_tokens.weight\",\n",
    "    \"embed_tokens.weight\",\n",
    "    \"model.decoder.embed_tokens.weight\",\n",
    "    \"lm_head.weight\"\n",
    "]\n",
    "\n",
    "target_key = next((k for k in CAND_KEYS if k in index[\"weight_map\"]), None)\n",
    "assert target_key is not None, \"Cannot find embedding weight key in index.json\"\n",
    "shard_file = index[\"weight_map\"][target_key]\n",
    "shard_path = hf_hub_download(repo_id=\"microsoft/LLM2CLIP-Llama-3-8B-Instruct-CC-Finetuned\", filename=shard_file)\n",
    "\n",
    "\n",
    "with safe_open(shard_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "    E = f.get_tensor(target_key)  # [V, D]\n",
    "print(\"Embedding shape:\", tuple(E.shape), E.dtype)\n",
    "\n",
    "\n",
    "E16 = E.half()\n",
    "E = None  # release\n",
    "\n",
    "def nn_for_token_str(tok, token_str, k=10):\n",
    "    tid = tok.convert_tokens_to_ids(token_str)\n",
    "    if tid == tok.unk_token_id:\n",
    "        print(f\"[warn] '{token_str}' is <unk> for this tokenizer.\")\n",
    "        return []\n",
    "    v = E16[tid].float().unsqueeze(0)           # [1, D] -> float32\n",
    "    v = F.normalize(v, dim=-1)\n",
    "    emb_norm = F.normalize(E16.float(), dim=-1) # [V, D] float32\n",
    "    sims = torch.matmul(v, emb_norm.t()).squeeze(0)  # [V]\n",
    "    topk = torch.topk(sims, k=k)\n",
    "    ids = topk.indices.tolist()\n",
    "    vals = topk.values.tolist()\n",
    "    return [(tok.convert_ids_to_tokens(i), float(s)) for i, s in zip(ids, vals)]\n",
    "\n",
    "targets = [\"Hel\", \"anism\"]\n",
    "for t in targets:\n",
    "    nns = nn_for_token_str(tok, t, k=10)\n",
    "    if not nns:\n",
    "        continue\n",
    "    print(f\"\\nTop-10 neighbors for '{t}':\")\n",
    "    for nn_tok, score in nns:\n",
    "        print(f\"  {nn_tok:>20s}  {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
